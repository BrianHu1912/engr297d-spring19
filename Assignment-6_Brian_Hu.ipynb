{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Import MNIST data\n",
    "#http://yann.lecun.com/exdb/mnist/\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO-DO : reset session\n",
    "def tf_reset():\n",
    "    try:\n",
    "        sess.close()\n",
    "    except:\n",
    "        pass\n",
    "    tf.reset_default_graph()\n",
    "    return tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_bias(n_classes):\n",
    "    w = {\n",
    "        \"w1\" : tf.get_variable('W0', shape=(5,5,1,32), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "        \"w2\" : tf.get_variable('W1', shape=(5,5,32,64), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "        'wd1': tf.get_variable('W3', shape=(7*7*64,1024), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "        'out': tf.get_variable('W4', shape=(1024,n_classes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    }\n",
    "\n",
    "    b = {\n",
    "        \"b1\" : tf.get_variable('B0', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "        \"b2\" : tf.get_variable('B1', shape=(64), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "        \"b4\" : tf.get_variable('B3', shape=(1024), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "        \"b5\" : tf.get_variable('B4', shape=(n_classes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    }\n",
    "\n",
    "    return w,b\n",
    "\n",
    "def conv2d(X,w,b,strides=1):\n",
    "    #building conv-layer\n",
    "    x = tf.nn.conv2d(X,w,strides=[1,strides,strides,1],padding=\"SAME\")\n",
    "    x = tf.nn.bias_add(x,b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(X, k=2):\n",
    "    return tf.nn.max_pool(X, ksize=[1, k, k, 1], strides=[1, k, k, 1],padding='SAME')\n",
    "\n",
    "def fully_connected(X,w,b):\n",
    "    fc = tf.reshape(X, [-1,w.get_shape().as_list()[0]])\n",
    "    fc = tf.matmul(fc, w)\n",
    "    fc = tf.add(fc,b)\n",
    "    return tf.nn.relu(fc)\n",
    "\n",
    "def build_model(input_layer,w,b):\n",
    "\n",
    "    input_layer = tf.reshape(input_layer,[-1,28,28,1])\n",
    "\n",
    "    #conv-layer-1\n",
    "    conv1 = conv2d(input_layer,w['w1'],b['b1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    print(conv1.get_shape())\n",
    "\n",
    "    conv2 = conv2d(conv1,w['w2'],b['b2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    print(conv2.get_shape())\n",
    "\n",
    "    fc1 = fully_connected(conv2,w['wd1'],b['b4'])\n",
    "    print(fc1.get_shape())\n",
    "\n",
    "    out = tf.matmul(fc1,w['out'])\n",
    "\n",
    "    return tf.add(out,b['b5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 14, 14, 32)\n",
      "(?, 7, 7, 64)\n",
      "(?, 1024)\n",
      "iteration : 0, loss : 2.27, acc : 0.16\n",
      "iteration : 20, loss : 1.74, acc : 0.52\n",
      "iteration : 40, loss : 0.54, acc : 0.86\n",
      "iteration : 60, loss : 0.26, acc : 0.95\n",
      "iteration : 80, loss : 0.29, acc : 0.91\n",
      "iteration : 100, loss : 0.20, acc : 0.95\n",
      "iteration : 120, loss : 0.11, acc : 0.97\n",
      "iteration : 140, loss : 0.06, acc : 0.99\n",
      "iteration : 160, loss : 0.12, acc : 0.98\n",
      "iteration : 180, loss : 0.17, acc : 0.95\n"
     ]
    }
   ],
   "source": [
    "#TO-DO \n",
    "# reset session\n",
    "sess = tf_reset()\n",
    "\n",
    "w,b = get_weights_bias(10)\n",
    "\n",
    "#TO-DO\n",
    "#Define input_placeholder #(None x 784)\n",
    "#Define output_placeholder #(None x 10)\n",
    "\n",
    "input_ph = tf.placeholder(dtype = tf.float32, shape=[None,784])\n",
    "output_ph = tf.placeholder(dtype=tf.float32, shape=[None,10])\n",
    "\n",
    "output_pred = build_model(input_ph,w,b)\n",
    "\n",
    "# Define Loss function # Not reduce Mean for linear regression. Look up lecture 7. Look up AdamOptimizer\n",
    "# Define optimizer\n",
    "# Initialize your variables \n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output_pred,labels=output_ph))\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(loss)\n",
    "#Using Gradient Descent for now. Will try to understand Adam Optimizer before the project. \n",
    "\n",
    "pred = tf.nn.softmax(output_pred)\n",
    "actual = tf.equal(tf.argmax(pred,1),tf.argmax(output_ph,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(actual,tf.float32))\n",
    "#I had to google this. \n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#Begin for loop:\n",
    "#get slice of input,output using mnist next batch\n",
    "#run session to compute loss\n",
    "#print loss w.r.t current iteration value\n",
    "\n",
    "for i in range(200):\n",
    "    input_slice,output_slice = mnist.train.next_batch(batch_size)\n",
    "    _,mse = sess.run([opt,loss],feed_dict={input_ph:input_slice,output_ph:output_slice})\n",
    "    #This part is from the last lab\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        mse,acc = sess.run([loss,accuracy],feed_dict={input_ph:input_slice,output_ph:output_slice})\n",
    "        print(\"iteration : {}, loss : {:.2f}, acc : {:.2f}\".format(i,mse,acc))\n",
    "        #This code is from lab 2. \n",
    "        # We aren't going to be printing all 200 iterations so we will do multiples of 20. 10 prints should be enough.\n",
    "        \n",
    "\n",
    "##try it out : compute accuracy using test\n",
    "#hint : minist.test gives you the test value\n",
    "# use only a part of the test dataset(slice first 256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
